# -*- coding: utf-8 -*-
"""NLP/NLU

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJZR4E0KAcvIgtuuZ32C6U5pTD9n2Eqb
"""

#basic data management libraries
import pandas as pd
import numpy as np
import pylab as pl
import re
from urllib.request import urlopen


import nltk 
import string
import en_core_web_sm
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import wordpunct_tokenize
from nltk.text import Text
from textblob import TextBlob
from nltk.corpus import stopwords
from gensim import corpora


nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Get to link to csv
url = 'https://drive.google.com/open?id=1tGR408tHc-3rgUX9TdAhtE2xqLviqMGO'

fluff, id = url.split('=')
print (id) # Verify that you have everything after '='

# Store in a Pandas Dataframe
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('test.csv')  
df = pd.read_csv('test.csv')
df.head()

df = df[:10]
df

text = df['Statement'][:10]
text

stopwords.words('english')

for w_index in text:
    print(wordpunct_tokenize(w_index))

# Creating new column to tokenize text
df['tokenized_statement'] = df.apply(lambda row: nltk.word_tokenize(row['Statement']), axis=1)
df

df['tokenized_statement']

# Stopwords for the tokenized words
stop = set(stopwords.words('english'))
exclude = set(string.punctuation) 
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized

lemmatized = [clean(doc).split() for doc in df['Statement']]
lemmatized

# Create dictionary and term matrix
dictionary = corpora.Dictionary(lemmatized)
doc_term_matrix = [dictionary.doc2bow(doc) for doc in lemmatized]
doc_term_matrix